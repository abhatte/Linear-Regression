#  Introduction

## Learning objectives:

1. Learn the R formula interface
2. Specify factor contrasts to test specific hypotheses
3. Perform model comparisons
4. Run and interpret variety of regression models in R

where am I?

```{r, echo=TRUE}
getwd() 
```

You might also start by listing the files in your working directory

```{r, echo=TRUE}
list.files("dataSets")
```

Load the states data
```{r, echo=TRUE}
setwd("/Users/ashwinibhatte/Documents/Data Science/linear_regression")
states.data <- readRDS("/Users/ashwinibhatte/Documents/Data Science/linear_regression/dataSets/states.rds") 
states.info <- data.frame(attributes(states.data)[c(
  "names", "var.labels")])
tail(states.info, 8)
```

Examine the data before fitting models

```{r, echo=TRUE}
summary(states.data)
```

Start by examining the data to check for problems.

summary of expense and csat columns, all rows

```{r, echo=TRUE}
sts.ex.sat <- subset(states.data, select = c("expense", "csat"))
summary(sts.ex.sat)
```

correlation between expense and csat
```{r, echo=TRUE}
cor(sts.ex.sat)
```

Plot the data before fitting models

```{r, echo=TRUE}
plot(sts.ex.sat)
```

Plot the data to look for multivariate outliers, non-linear relationships etc.

scatter plot of expense vs csat

```{r, echo=TRUE}
plot(sts.ex.sat)
```

Linear regression example

```{r, echo=TRUE}
lm(csat~expense, data = states.data)
```

Linear regression models can be fit with the `lm()' function.

For example, we can use `lm' to predict SAT scores based on per-pupal expenditures:

## Fit our regression model

```{r, echo=TRUE}
sat.mod <- lm(csat ~ expense, # regression formula
              data=states.data) # data set
```

Summarize and print the results

```{r, echo=TRUE}
summary(sat.mod) # show regression coefficients table
```

Why is the association between expense and SAT scores /negative/?

Many people find it surprising that the per-capita expenditure on students is negatively related to SAT scores. The beauty of multiple regression is that we can try to pull these apart. 

What would the association between expense and SAT scores be if there were no difference among the states in the percentage of students taking the SAT?

```{r, echo=TRUE}
summary(lm(csat ~ expense + percent, data = states.data))
```

## The lm class and methods

```{r, echo=TRUE}
class(lm(csat ~ expense + percent, data = states.data))
```

OK, we fit our model. Now what? 

## Examine the model object:

```{r, echo=TRUE}
class(sat.mod)
names(sat.mod)
methods(class = class(sat.mod))[1:9]
```

Use function methods to get more information about the fit

```{r, echo=TRUE}
confint(sat.mod)
hist(residuals(sat.mod))
```

## Linear Regression Assumptions

Ordinary least squares regression relies on several assumptions, including that the residuals are normally distributed and homoscedastic, the errors are independent and the relationships are linear.

Investigate these assumptions visually by plotting your model:

```{r, echo=TRUE}
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2)) #optional

plot(sat.mod, which = c(1, 2)) # "which" argument optional
```

## Comparing models

Do congressional voting patterns predict SAT scores over and above expense? 

Fit two models and compare them:

fit another model, adding house and senate as predictors

```{r, echo=TRUE}
sat.voting.mod <-  lm(csat ~ expense + house + senate,
                      data = na.omit(states.data))
sat.mod <- update(sat.mod, data=na.omit(states.data))
```

compare using the anova() function

```{r, echo=TRUE}
anova(sat.mod, sat.voting.mod)
coef(summary(sat.voting.mod))
```


## _Exercise: least squares regression_

Use the /states.rds/ data set. Fit a model predicting energy consumed per capita (energy) from the percentage of residents living in metropolitan areas (metro). Be sure to

1. Examine/plot the data before fitting the model

2. Print and interpret the model `summary'

3. `plot' the model to look for deviations from modeling assumptions


Select one or more additional predictors to add to your model and repeat steps 1-3. Is this model significantly better than the model with /metro/ as the only predictor?

```{r, echo=TRUE}
sts.en.met <- subset(states.data, select = c("metro", "energy"))
summary(sts.en.met)
# correlation
cor(sts.en.met)
```

scatter plot before fitting the model

```{r, echo=TRUE}
plot(sts.en.met)
```

Fit our regression model

```{r, echo=TRUE}
en.mod <- lm(energy ~ metro, # regression formula
              data=states.data) # data set
```

# Summarize and print the results

```{r, echo=TRUE}
summary(en.mod) # show regression coefficients table
par(mar = c(4, 4, 2, 2), mfrow = c(1, 2)) #optional
plot(en.mod, which = c(1, 2)) # "which" argument optional

income.mod <-  lm(energy ~ metro + income ,
                      data = na.omit(states.data))
en.mod <- update(en.mod, data=na.omit(states.data))
```


# compare using the anova() function

```{r, echo=TRUE}
anova(en.mod, income.mod)
coef(summary(income.mod))
```

## Interactions and factors

Modeling interactions

Interactions allow us assess the extent to which the association between one predictor and the outcome depends on a second predictor.

For example: Does the association between expense and SAT scores depend on the median income in the state?

Add the interaction to the model

```{r, echo=TRUE}
sat.expense.by.percent <- lm(csat ~ expense*income,
                             data=states.data) 
```

Show the results

```{r, echo=TRUE}
coef(summary(sat.expense.by.percent))
```

## Regression with categorical predictors

Let's try to predict SAT scores from region, a categorical variable. Note that you must make sure R does not think your categorical variable is numeric.

make sure R knows region is categorical

```{r, echo=TRUE}
str(states.data$region)
states.data$region <- factor(states.data$region)
```

Add region to the model

```{r, echo=TRUE}
sat.region <- lm(csat ~ region,
                 data=states.data) 
```

Show the results

```{r, echo=TRUE}
coef(summary(sat.region)) # show regression coefficients table

anova(sat.region) # show ANOVA table
```

Again, *make sure to tell R which variables are categorical by converting them to factors!*

Setting factor reference groups and contrasts

In the previous example we use the default contrasts for region. The default in R is treatment contrasts, with the first level as the reference. We can change the reference group or use another coding scheme using the `C' function.

print default contrasts
 
```{r, echo=TRUE}
contrasts(states.data$region)
```

change the reference group

```{r, echo=TRUE}
coef(summary(lm(csat ~ C(region, base=4), 
                data=states.data)))
```

See also `?contrasts', `?contr.treatment', and `?relevel'.

## Exercise: interactions and factors

Use the states data set.


1. Add on to the regression equation that you created in exercise 1 by generating an interaction term and testing the interaction.


2. Try adding region to the model. Are there significant differences across the four regions?

### Add interactions to the model

```{r, echo=TRUE}
sat.income.by.percent <- lm(energy ~ metro*income,
                            data=states.data) 
coef(summary(sat.income.by.percent))

sat.region2 <- lm(energy ~ region,
                  data=states.data) 
```

Show the results

```{r, echo=TRUE}
coef(summary(sat.region2)) # show regression coefficients table

anova(sat.region2) # show ANOVA table
```


```{r, echo=TRUE}
# print default contrasts
contrasts(states.data$region) 
# change the reference group
coef(summary(lm(energy ~ C(region, base=4), 
                data=states.data)))

# change the coding scheme
coef(summary(lm(energy ~ C(region, contr.helmert),
                data=states.data)))
```








































